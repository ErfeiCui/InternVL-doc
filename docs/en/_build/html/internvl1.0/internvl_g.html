

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>InternVL Stage-2 Pre-training &amp; Retrieval Fine-tuning &#8212; internvl</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/readthedocs.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'internvl1.0/internvl_g';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="InternVL for Multimodal Dialogue using LLaVA Codebase" href="internvl_chat_llava.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Aug 06, 2024"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/internvl-logo.svg" class="logo__image only-light" alt="internvl - Home"/>
    <script>document.write(`<img src="../_static/internvl-logo.svg" class="logo__image only-dark" alt="internvl - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/eval_data_preparation.html">Evaluation Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/chat_data_format.html">Chat Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/internvl_chat_api.html">InternVL-Chat API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/local_chat_demo.html">Local Chat Demo</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tutorials/coco_caption_finetune.html">Enhancing InternVL2 on COCO Caption Using LoRA Fine-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/classic_questions.html">Classic Questions &amp; Issues</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternVL 2.0</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../internvl2.0/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl2.0/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl2.0/finetune.html">Finetune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl2.0/evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl2.0/deployment.html">Deployment</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternVL 1.5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../internvl1.5/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.5/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.5/finetune.html">Finetune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.5/evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.5/deployment.html">Deployment</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternVL 1.2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../internvl1.2/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.2/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.2/reproduce.html">Reproduce</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.2/finetune.html">Finetune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.2/evaluation.html">Evaluation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternVL 1.1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../internvl1.1/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.1/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internvl1.1/evaluation.html">Evaluation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternVL 1.0</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classification.html">classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="clip_benchmark.html">clip_benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="segmentation.html">segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="internvl_chat_llava.html">internvl_chat_llava</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">internvl_g</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/OpenGVLab/InternVL" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/OpenGVLab/InternVL/blob/main/docs/en/internvl1.0/internvl_g.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/OpenGVLab/InternVL/edit/main/docs/en/internvl1.0/internvl_g.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/OpenGVLab/InternVL/issues/new?title=Issue%20on%20page%20%2Finternvl1.0/internvl_g.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/internvl1.0/internvl_g.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>InternVL Stage-2 Pre-training & Retrieval Fine-tuning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">Data Preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-preparation">Model Preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-pre-training">Generative Pre-training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-image-captioning">Zero-Shot Image Captioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuned-image-text-retrieval">Fine-tuned Image-Text Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flickr30k-fine-tuned-model-internvl-14b-flickr30k-ft-364px">Flickr30K fine-tuned model: InternVL-14B-Flickr30K-FT-364px</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flickr30k-cn-fine-tuned-model-internvl-14b-flickrcn-ft-364px">Flickr30K-CN fine-tuned model: InternVL-14B-FlickrCN-FT-364px</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-fine-tuning-fully">Retrieval Fine-tuning (Fully)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-fine-tuning-head">Retrieval Fine-tuning (Head)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-fine-tuning-lora">Retrieval Fine-tuning (LoRA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-a-custom-dataset">Fine-Tuning a Custom Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citation">Citation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="internvl-stage-2-pre-training-retrieval-fine-tuning">
<h1>InternVL Stage-2 Pre-training &amp; Retrieval Fine-tuning<a class="headerlink" href="#internvl-stage-2-pre-training-retrieval-fine-tuning" title="Permalink to this heading">#</a></h1>
<p>This folder contains the implementation of the InternVL 1.0 for stage2 pre-training and retrieval fine-tuning, which corresponds to Section 4.3 of our <a class="reference external" href="https://arxiv.org/pdf/2312.14238">InternVL 1.0 paper</a>.</p>
<p><img alt="image" src="../_images/internvl_g.png" /></p>
<section id="data-preparation">
<h2>Data Preparation<a class="headerlink" href="#data-preparation" title="Permalink to this heading">#</a></h2>
<p>Three datasets need to be prepared: COCO Caption, Flickr30K, and NoCaps.</p>
<details open>
<summary>COCO Caption</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>data/coco<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>data/coco

<span class="c1"># download coco images</span>
wget<span class="w"> </span>http://images.cocodataset.org/zips/train2014.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>unzip<span class="w"> </span>train2014.zip
wget<span class="w"> </span>http://images.cocodataset.org/zips/val2014.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>unzip<span class="w"> </span>val2014.zip
wget<span class="w"> </span>http://images.cocodataset.org/zips/test2015.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>unzip<span class="w"> </span>test2015.zip

mkdir<span class="w"> </span>-p<span class="w"> </span>annotations<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>annotations/
<span class="c1"># download converted annotation files</span>
wget<span class="w"> </span>https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json
wget<span class="w"> </span>https://github.com/OpenGVLab/InternVL/releases/download/data/coco_karpathy_test.json
wget<span class="w"> </span>https://github.com/OpenGVLab/InternVL/releases/download/data/coco_karpathy_test_gt.json
<span class="nb">cd</span><span class="w"> </span>../../../
</pre></div>
</div>
</details>
<details open>
<summary>Flickr30K</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>data/flickr30k<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>data/flickr30k

<span class="c1"># download images from https://bryanplummer.com/Flickr30kEntities/</span>
<span class="c1"># karpathy split annotations can be downloaded from the following link:</span>
<span class="c1"># https://github.com/mehdidc/retrieval_annotations/releases/download/1.0.0/flickr30k_test_karpathy.txt</span>
<span class="c1"># this file is provided by the clip-benchmark repository.</span>
<span class="c1"># We convert this txt file to json format, download the converted file:</span>
wget<span class="w"> </span>https://github.com/OpenGVLab/InternVL/releases/download/data/flickr30k_cn_test.txt
wget<span class="w"> </span>https://github.com/OpenGVLab/InternVL/releases/download/data/flickr30k_cn_train.txt
wget<span class="w"> </span>https://github.com/OpenGVLab/InternVL/releases/download/data/flickr30k_test_karpathy.json
wget<span class="w"> </span>https://github.com/mehdidc/retrieval_annotations/releases/download/1.0.0/flickr30k_test_karpathy.txt
wget<span class="w"> </span>https://github.com/mehdidc/retrieval_annotations/releases/download/1.0.0/flickr30k_train_karpathy.txt
wget<span class="w"> </span>https://github.com/mehdidc/retrieval_annotations/releases/download/1.0.0/flickr30k_val_karpathy.txt

<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
</details>
<details open>
<summary>NoCaps</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>data/nocaps<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>data/nocaps

<span class="c1"># download images from https://nocaps.org/download</span>
<span class="c1"># original annotations can be downloaded from https://nocaps.s3.amazonaws.com/nocaps_val_4500_captions.json</span>
wget<span class="w"> </span>https://nocaps.s3.amazonaws.com/nocaps_val_4500_captions.json

<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
</details>
<p>After the download is complete, the directory structure is:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>data
├──<span class="w"> </span>coco
│<span class="w">   </span>├──<span class="w"> </span>annotations
│<span class="w">   </span>│<span class="w">   </span>├──<span class="w"> </span>coco_karpathy_train.json
│<span class="w">   </span>├──<span class="w"> </span>test2017
│<span class="w">   </span>├──<span class="w"> </span>train2014
│<span class="w">   </span>├──<span class="w"> </span>train2017
│<span class="w">   </span>├──<span class="w"> </span>val2014
│<span class="w">   </span>└──<span class="w"> </span>val2017
├──<span class="w"> </span>flickr30k
│<span class="w">   </span>├──<span class="w"> </span>flickr30k_cn_test.txt
│<span class="w">   </span>├──<span class="w"> </span>flickr30k_cn_train.txt
│<span class="w">   </span>├──<span class="w"> </span>flickr30k_test_karpathy.json
│<span class="w">   </span>├──<span class="w"> </span>flickr30k_test_karpathy.txt
│<span class="w">   </span>├──<span class="w"> </span>flickr30k_train_karpathy.txt
│<span class="w">   </span>├──<span class="w"> </span>flickr30k_val_karpathy.txt
│<span class="w">   </span>└──<span class="w"> </span>Images
└──<span class="w"> </span>nocaps
<span class="w">    </span>├──<span class="w"> </span>images
<span class="w">    </span>└──<span class="w"> </span>nocaps_val_4500_captions.json
</pre></div>
</div>
</section>
<section id="model-preparation">
<h2>Model Preparation<a class="headerlink" href="#model-preparation" title="Permalink to this heading">#</a></h2>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>model name</p></th>
<th class="head"><p>type</p></th>
<th class="head"><p>param</p></th>
<th class="head"><p>download</p></th>
<th class="head text-center"><p>size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>InternVL-14B-224px</p></td>
<td><p>huggingface</p></td>
<td><p>13.8B</p></td>
<td><p>🤗 <a class="reference external" href="https://huggingface.co/OpenGVLab/InternVL-14B-224px">HF link</a></p></td>
<td class="text-center"><p>27.7 GB</p></td>
</tr>
</tbody>
</table>
<p>Download the above model weights and place them in the <code class="docutils literal notranslate"><span class="pre">pretrained/</span></code> folder.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>pretrained/
<span class="c1"># pip install -U huggingface_hub</span>
huggingface-cli<span class="w"> </span>download<span class="w"> </span>--resume-download<span class="w"> </span>--local-dir-use-symlinks<span class="w"> </span>False<span class="w"> </span>OpenGVLab/InternVL-14B-224px<span class="w"> </span>--local-dir<span class="w"> </span>InternVL-14B-224px
</pre></div>
</div>
<p>The directory structure is:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pretrained
└──<span class="w"> </span>InternVL-14B-224px/
</pre></div>
</div>
</section>
<section id="generative-pre-training">
<h2>Generative Pre-training<a class="headerlink" href="#generative-pre-training" title="Permalink to this heading">#</a></h2>
<p>There are currently no plans to release this part of the code.</p>
</section>
<section id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this heading">#</a></h2>
<section id="zero-shot-image-captioning">
<h3>Zero-Shot Image Captioning<a class="headerlink" href="#zero-shot-image-captioning" title="Permalink to this heading">#</a></h3>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>model</p></th>
<th class="head"><p>dataset</p></th>
<th class="head"><p>BLEU4</p></th>
<th class="head"><p>METEOR</p></th>
<th class="head"><p>CIDEr</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>InternVL-G</p></td>
<td><p>COCO Karpathy test</p></td>
<td><p>37.1</p></td>
<td><p>30.1</p></td>
<td><p>128.2</p></td>
</tr>
<tr class="row-odd"><td><p>InternVL-G</p></td>
<td><p>Flickr30K Karpathy test</p></td>
<td><p>27.0</p></td>
<td><p>25.3</p></td>
<td><p>79.2</p></td>
</tr>
<tr class="row-even"><td><p>InternVL-G</p></td>
<td><p>NoCaps val</p></td>
<td><p>44.3</p></td>
<td><p>30.1</p></td>
<td><p>113.7</p></td>
</tr>
</tbody>
</table>
<details>
  <summary>[InternVL-G] COCO Karpathy test</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sh<span class="w"> </span>evaluate.sh<span class="w"> </span>pretrained/InternVL-14B-224px<span class="w"> </span>caption-coco
</pre></div>
</div>
<p>Expected results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;coco&#39;</span><span class="p">,</span> <span class="s1">&#39;English caption:&#39;</span><span class="p">,</span> <span class="mf">10.5974</span><span class="p">,</span> <span class="n">dict_items</span><span class="p">([(</span><span class="s1">&#39;Bleu_1&#39;</span><span class="p">,</span> <span class="mf">0.7876323287981284</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Bleu_2&#39;</span><span class="p">,</span> <span class="mf">0.6353512494727918</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Bleu_3&#39;</span><span class="p">,</span> <span class="mf">0.49108984183589743</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Bleu_4&#39;</span><span class="p">,</span> <span class="mf">0.37062736733849205</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;METEOR&#39;</span><span class="p">,</span> <span class="mf">0.30106315496945923</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;ROUGE_L&#39;</span><span class="p">,</span> <span class="mf">0.5898249189475652</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;CIDEr&#39;</span><span class="p">,</span> <span class="mf">1.281844384075423</span><span class="p">)])]</span>
</pre></div>
</div>
</details>
<details>
  <summary>[InternVL-G] Flickr30K Karpathy test</summary>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sh</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">sh</span> <span class="n">pretrained</span><span class="o">/</span><span class="n">InternVL</span><span class="o">-</span><span class="mi">14</span><span class="n">B</span><span class="o">-</span><span class="mi">224</span><span class="n">px</span> <span class="n">caption</span><span class="o">-</span><span class="n">flickr30k</span>
</pre></div>
</div>
<p>Expected results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span><span class="s1">&#39;flickr30k&#39;</span>,<span class="w"> </span><span class="s1">&#39;English caption:&#39;</span>,<span class="w"> </span><span class="m">10</span>.666,<span class="w"> </span>dict_items<span class="o">([(</span><span class="s1">&#39;Bleu_1&#39;</span>,<span class="w"> </span><span class="m">0</span>.7182900534357628<span class="o">)</span>,<span class="w"> </span><span class="o">(</span><span class="s1">&#39;Bleu_2&#39;</span>,<span class="w"> </span><span class="m">0</span>.5353390037921949<span class="o">)</span>,<span class="w"> </span><span class="o">(</span><span class="s1">&#39;Bleu_3&#39;</span>,<span class="w"> </span><span class="m">0</span>.3834462132295285<span class="o">)</span>,<span class="w"> </span><span class="o">(</span><span class="s1">&#39;Bleu_4&#39;</span>,<span class="w"> </span><span class="m">0</span>.2702131471765472<span class="o">)</span>,<span class="w"> </span><span class="o">(</span><span class="s1">&#39;METEOR&#39;</span>,<span class="w"> </span><span class="m">0</span>.25263515267930103<span class="o">)</span>,<span class="w"> </span><span class="o">(</span><span class="s1">&#39;ROUGE_L&#39;</span>,<span class="w"> </span><span class="m">0</span>.5305876871149064<span class="o">)</span>,<span class="w"> </span><span class="o">(</span><span class="s1">&#39;CIDEr&#39;</span>,<span class="w"> </span><span class="m">0</span>.7919734768328237<span class="o">)])]</span>
</pre></div>
</div>
</details>
<details>
  <summary>[InternVL-G] NoCaps val</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sh<span class="w"> </span>evaluate.sh<span class="w"> </span>pretrained/InternVL-14B-224px<span class="w"> </span>caption-nocaps
</pre></div>
</div>
<p>Expected results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;nocaps&#39;</span><span class="p">,</span> <span class="s1">&#39;English caption:&#39;</span><span class="p">,</span> <span class="mf">10.463111111111111</span><span class="p">,</span> <span class="n">dict_items</span><span class="p">([(</span><span class="s1">&#39;Bleu_1&#39;</span><span class="p">,</span> <span class="mf">0.8518290482155187</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Bleu_2&#39;</span><span class="p">,</span> <span class="mf">0.7165227921485106</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Bleu_3&#39;</span><span class="p">,</span> <span class="mf">0.5733723839888316</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Bleu_4&#39;</span><span class="p">,</span> <span class="mf">0.44268902150723105</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;METEOR&#39;</span><span class="p">,</span> <span class="mf">0.30078174807736896</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;ROUGE_L&#39;</span><span class="p">,</span> <span class="mf">0.6070208063052156</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;CIDEr&#39;</span><span class="p">,</span> <span class="mf">1.1371742045267772</span><span class="p">)])]</span>
</pre></div>
</div>
</details>
</section>
<section id="fine-tuned-image-text-retrieval">
<h3>Fine-tuned Image-Text Retrieval<a class="headerlink" href="#fine-tuned-image-text-retrieval" title="Permalink to this heading">#</a></h3>
<section id="flickr30k-fine-tuned-model-internvl-14b-flickr30k-ft-364px">
<h4>Flickr30K fine-tuned model: <a class="reference external" href="https://huggingface.co/OpenGVLab/InternVL-14B-Flickr30K-FT-364px">InternVL-14B-Flickr30K-FT-364px</a><a class="headerlink" href="#flickr30k-fine-tuned-model-internvl-14b-flickr30k-ft-364px" title="Permalink to this heading">#</a></h4>
<table class="table">
  <tr align=center>
      <td rowspan="3" align=center><b>model</b></td>
      <td colspan="6" align=center><b>Flickr30K</b></td>
      <td rowspan="3" align=center><b>avg</b></td>
</tr>
   <tr align=center>
      <td colspan="3" align=center><b>image-to-text</b></td>
      <td colspan="3" align=center><b>text-to-image</b></td>
   </tr>
   <tr>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
   </tr>
<tr align=center>
      <td>InternVL-C-FT</td>
      <td>97.2</td>
      <td>100.0</td>
      <td>100.0</td>
      <td>88.5</td>
      <td>98.4</td>
      <td>99.2</td>
      <td>97.2</td>
   </tr>
<tr align=center>
      <td>InternVL-G-FT</td>
      <td>97.9</td>
      <td>100.0</td>
      <td>100.0</td>
      <td>89.6</td>
      <td>98.6</td>
      <td>99.2</td>
      <td>97.6</td>
   </tr>
</table>
<details>
  <summary>[InternVL-C-FT] Flickr30K</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>../clip_benchmark/
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python3<span class="w"> </span>clip_benchmark/cli.py<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>--model_type<span class="w"> </span>internvl<span class="w"> </span>--language<span class="w"> </span><span class="s2">&quot;en&quot;</span><span class="w"> </span>--task<span class="w"> </span><span class="s2">&quot;zeroshot_retrieval&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--dataset<span class="w"> </span><span class="s2">&quot;flickr30k&quot;</span><span class="w"> </span>--dataset_root<span class="w"> </span>./data/flickr30k<span class="w"> </span>--model<span class="w"> </span>internvl_c_retrieval_hf<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--pretrained<span class="w"> </span>./work_dirs/internvl_stage2_finetune_flickr_364_bs1024_ep10/<span class="w"> </span>--output<span class="w"> </span>result_ft.json
</pre></div>
</div>
<p>Expected results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;dataset&quot;</span><span class="p">:</span> <span class="s2">&quot;flickr30k&quot;</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;internvl_c_retrieval_hf&quot;</span><span class="p">,</span> <span class="s2">&quot;pretrained&quot;</span><span class="p">:</span> <span class="s2">&quot;./work_dirs/internvl_stage2_finetune_flickr_364_bs1024_ep10&quot;</span><span class="p">,</span> <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;zeroshot_retrieval&quot;</span><span class="p">,</span>
<span class="s2">&quot;metrics&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image_retrieval_recall@1&quot;</span><span class="p">:</span> <span class="mf">0.8853999972343445</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@1&quot;</span><span class="p">:</span> <span class="mf">0.972000002861023</span><span class="p">,</span>
<span class="s2">&quot;image_retrieval_recall@5&quot;</span><span class="p">:</span> <span class="mf">0.9836000204086304</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@5&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="s2">&quot;image_retrieval_recall@10&quot;</span><span class="p">:</span> <span class="mf">0.9923999905586243</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@10&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="s2">&quot;language&quot;</span><span class="p">:</span> <span class="s2">&quot;en&quot;</span><span class="p">}</span>
</pre></div>
</div>
</details>
<details>
  <summary>[InternVL-G-FT] Flickr30K</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>../clip_benchmark/
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python3<span class="w"> </span>clip_benchmark/cli.py<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>--model_type<span class="w"> </span>internvl<span class="w"> </span>--language<span class="w"> </span><span class="s2">&quot;en&quot;</span><span class="w"> </span>--task<span class="w"> </span><span class="s2">&quot;zeroshot_retrieval&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--dataset<span class="w"> </span><span class="s2">&quot;flickr30k&quot;</span><span class="w"> </span>--dataset_root<span class="w"> </span>./data/flickr30k<span class="w"> </span>--model<span class="w"> </span>internvl_g_retrieval_hf<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--pretrained<span class="w"> </span>./work_dirs/internvl_stage2_finetune_flickr_364_bs1024_ep10/<span class="w"> </span>--output<span class="w"> </span>result_ft.json
</pre></div>
</div>
<p>Expected results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;dataset&quot;</span><span class="p">:</span> <span class="s2">&quot;flickr30k&quot;</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;internvl_g_retrieval_hf&quot;</span><span class="p">,</span> <span class="s2">&quot;pretrained&quot;</span><span class="p">:</span> <span class="s2">&quot;./work_dirs/internvl_stage2_finetune_flickr_364_bs1024_ep10&quot;</span><span class="p">,</span> <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;zeroshot_retrieval&quot;</span><span class="p">,</span>
<span class="s2">&quot;metrics&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image_retrieval_recall@1&quot;</span><span class="p">:</span> <span class="mf">0.895799994468689</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@1&quot;</span><span class="p">:</span> <span class="mf">0.9789999723434448</span><span class="p">,</span>
<span class="s2">&quot;image_retrieval_recall@5&quot;</span><span class="p">:</span> <span class="mf">0.9861999750137329</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@5&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="s2">&quot;image_retrieval_recall@10&quot;</span><span class="p">:</span> <span class="mf">0.9922000169754028</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@10&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="s2">&quot;language&quot;</span><span class="p">:</span> <span class="s2">&quot;en&quot;</span><span class="p">}</span>
</pre></div>
</div>
</details>
</section>
<section id="flickr30k-cn-fine-tuned-model-internvl-14b-flickrcn-ft-364px">
<h4>Flickr30K-CN fine-tuned model: <a class="reference external" href="https://huggingface.co/OpenGVLab/InternVL-14B-FlickrCN-FT-364px">InternVL-14B-FlickrCN-FT-364px</a><a class="headerlink" href="#flickr30k-cn-fine-tuned-model-internvl-14b-flickrcn-ft-364px" title="Permalink to this heading">#</a></h4>
<table class="table">
  <tr align=center>
      <td rowspan="3" align=center><b>model</b></td>
      <td colspan="6" align=center><b>Flickr30K-CN</b></td>
      <td rowspan="3" align=center><b>avg</b></td>
</tr>
   <tr align=center>
       <td colspan="3" align=center><b>image-to-text</b></td>
      <td colspan="3" align=center><b>text-to-image</b></td>
   </tr>
   <tr>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
      <td>R@1</td>
      <td>R@5</td>
      <td>R@10</td>
   </tr>
<tr align=center>
      <td>InternVL-C-FT</td>
      <td>96.5</td>
      <td>99.9</td>
      <td>100.0</td>
      <td>85.2</td>
      <td>97.0</td>
      <td>98.5</td>
      <td>96.2</td>
   </tr>
<tr align=center>
      <td>InternVL-G-FT</td>
      <td>96.9</td>
      <td>99.9</td>
      <td>100.0</td>
      <td>85.9</td>
      <td>97.1</td>
      <td>98.7</td>
      <td>96.4</td>
   </tr>
</table>
<details>
  <summary>[InternVL-C-FT] Flickr30K-CN</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>../clip_benchmark/
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python3<span class="w"> </span>clip_benchmark/cli.py<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>--model_type<span class="w"> </span>internvl<span class="w"> </span>--language<span class="w"> </span><span class="s2">&quot;cn&quot;</span><span class="w"> </span>--task<span class="w"> </span><span class="s2">&quot;zeroshot_retrieval&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--dataset<span class="w"> </span><span class="s2">&quot;flickr30k&quot;</span><span class="w"> </span>--dataset_root<span class="w"> </span>./data/flickr30k<span class="w"> </span>--model<span class="w"> </span>internvl_c_retrieval_hf<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--pretrained<span class="w"> </span>./work_dirs/internvl_stage2_finetune_flickrcn_364_bs1024_ep10/<span class="w"> </span>--output<span class="w"> </span>result_ft.json
</pre></div>
</div>
<p>Expected results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;dataset&quot;</span><span class="p">:</span> <span class="s2">&quot;flickr30k&quot;</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;internvl_c_retrieval_hf&quot;</span><span class="p">,</span> <span class="s2">&quot;pretrained&quot;</span><span class="p">:</span> <span class="s2">&quot;./work_dirs/internvl_stage2_finetune_flickrcn_364_bs1024_ep10&quot;</span><span class="p">,</span> <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;zeroshot_retrieval&quot;</span><span class="p">,</span>
<span class="s2">&quot;metrics&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image_retrieval_recall@1&quot;</span><span class="p">:</span> <span class="mf">0.8521999716758728</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@1&quot;</span><span class="p">:</span> <span class="mf">0.9649999737739563</span><span class="p">,</span>
<span class="s2">&quot;image_retrieval_recall@5&quot;</span><span class="p">:</span> <span class="mf">0.9697999954223633</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@5&quot;</span><span class="p">:</span> <span class="mf">0.9990000128746033</span><span class="p">,</span>
<span class="s2">&quot;image_retrieval_recall@10&quot;</span><span class="p">:</span> <span class="mf">0.9854000210762024</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@10&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="s2">&quot;language&quot;</span><span class="p">:</span> <span class="s2">&quot;cn&quot;</span><span class="p">}</span>
</pre></div>
</div>
</details>
<details>
  <summary>[InternVL-G-FT] Flickr30K-CN</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>../clip_benchmark/
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python3<span class="w"> </span>clip_benchmark/cli.py<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>--model_type<span class="w"> </span>internvl<span class="w"> </span>--language<span class="w"> </span><span class="s2">&quot;cn&quot;</span><span class="w"> </span>--task<span class="w"> </span><span class="s2">&quot;zeroshot_retrieval&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--dataset<span class="w"> </span><span class="s2">&quot;flickr30k&quot;</span><span class="w"> </span>--dataset_root<span class="w"> </span>./data/flickr30k<span class="w"> </span>--model<span class="w"> </span>internvl_g_retrieval_hf<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--pretrained<span class="w"> </span>./work_dirs/internvl_stage2_finetune_flickrcn_364_bs1024_ep10/<span class="w"> </span>--output<span class="w"> </span>result_ft.json
</pre></div>
</div>
<p>Expected results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;dataset&quot;</span><span class="p">:</span> <span class="s2">&quot;flickr30k&quot;</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;internvl_g_retrieval_hf&quot;</span><span class="p">,</span> <span class="s2">&quot;pretrained&quot;</span><span class="p">:</span> <span class="s2">&quot;./work_dirs/internvl_stage2_finetune_flickrcn_364_bs1024_ep10&quot;</span><span class="p">,</span> <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;zeroshot_retrieval&quot;</span><span class="p">,</span>
<span class="s2">&quot;metrics&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image_retrieval_recall@1&quot;</span><span class="p">:</span> <span class="mf">0.8587999939918518</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@1&quot;</span><span class="p">:</span> <span class="mf">0.968999981880188</span><span class="p">,</span>
<span class="s2">&quot;image_retrieval_recall@5&quot;</span><span class="p">:</span> <span class="mf">0.9714000225067139</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@5&quot;</span><span class="p">:</span> <span class="mf">0.9990000128746033</span><span class="p">,</span>
<span class="s2">&quot;image_retrieval_recall@10&quot;</span><span class="p">:</span> <span class="mf">0.9865999817848206</span><span class="p">,</span> <span class="s2">&quot;text_retrieval_recall@10&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="s2">&quot;language&quot;</span><span class="p">:</span> <span class="s2">&quot;cn&quot;</span><span class="p">}</span>
</pre></div>
</div>
</details>
</section>
</section>
</section>
<section id="retrieval-fine-tuning-fully">
<h2>Retrieval Fine-tuning (Fully)<a class="headerlink" href="#retrieval-fine-tuning-fully" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>Note: In our experiments, full parameter fine-tuning achieves the best results on image-text retrieval tasks in Flickr30K and COCO. By following the experimental hyperparameters in this section, you can reproduce the model performance reported in the <a class="reference internal" href="#evaluation">Evaluation section</a>.</p>
</div></blockquote>
<p>To fine-tune InternVL on Flickr30K with 32 GPUs and slurm system, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">PARTITION</span><span class="o">=</span><span class="s1">&#39;your partition&#39;</span><span class="w"> </span><span class="nv">GPUS</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>sh<span class="w"> </span>shell/finetune/internvl_stage2_finetune_flickr_364_bs1024_ep10.sh
</pre></div>
</div>
<p>To fine-tune InternVL on Flickr30K-CN with 32 GPUs and slurm system, run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PARTITION</span><span class="o">=</span><span class="s1">&#39;your partition&#39;</span><span class="w"> </span><span class="nv">GPUS</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>sh<span class="w"> </span>shell/finetune/internvl_stage2_finetune_flickrcn_364_bs1024_ep10.sh
</pre></div>
</div>
<p>To fine-tune InternVL on COCO with 32 GPUs and slurm system, run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PARTITION</span><span class="o">=</span><span class="s1">&#39;your partition&#39;</span><span class="w"> </span><span class="nv">GPUS</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>sh<span class="w"> </span>shell/finetune/internvl_stage2_finetune_coco_364_bs1024_ep5.sh
</pre></div>
</div>
<p>The hyperparameters used here are:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>config</p></th>
<th class="head"><p>Flickr30K</p></th>
<th class="head"><p>Flickr30K-CN</p></th>
<th class="head"><p>COCO</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>learning rate</p></td>
<td><p>1e-6</p></td>
<td><p>1e-6</p></td>
<td><p>1e-6</p></td>
</tr>
<tr class="row-odd"><td><p>layer-wise lr<br>decay rate</p></td>
<td><p>InternViT-6B (0.9),<br>QLLaMA (0.9)</p></td>
<td><p>InternViT-6B (0.9),<br>QLLaMA (0.9)</p></td>
<td><p>InternViT-6B (0.9),<br>QLLaMA (0.9)</p></td>
</tr>
<tr class="row-even"><td><p>optimizer</p></td>
<td><p>AdamW</p></td>
<td><p>AdamW</p></td>
<td><p>AdamW</p></td>
</tr>
<tr class="row-odd"><td><p>weight decay</p></td>
<td><p>0.05</p></td>
<td><p>0.05</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-even"><td><p>input resolution</p></td>
<td><p>364x364</p></td>
<td><p>364x364</p></td>
<td><p>364x364</p></td>
</tr>
<tr class="row-odd"><td><p>total batch size</p></td>
<td><p>1024</p></td>
<td><p>1024</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-even"><td><p>warm-up iterations</p></td>
<td><p>100</p></td>
<td><p>100</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-odd"><td><p>training epochs</p></td>
<td><p>10</p></td>
<td><p>10</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-even"><td><p>drop path rate</p></td>
<td><p>0.3</p></td>
<td><p>0.3</p></td>
<td><p>0.3</p></td>
</tr>
<tr class="row-odd"><td><p>numerical precision</p></td>
<td><p>zero1 + bf16</p></td>
<td><p>zero1 + bf16</p></td>
<td><p>zero1 + bf16</p></td>
</tr>
<tr class="row-even"><td><p>trainable / total params</p></td>
<td><p>14B / 14B</p></td>
<td><p>14B / 14B</p></td>
<td><p>14B / 14B</p></td>
</tr>
<tr class="row-odd"><td><p>GPUs for training</p></td>
<td><p>32×A100 (80G)</p></td>
<td><p>32×A100 (80G)</p></td>
<td><p>32×A100 (80G)</p></td>
</tr>
<tr class="row-even"><td><p>Required GPU memory</p></td>
<td><p>80G</p></td>
<td><p>80G</p></td>
<td><p>80G</p></td>
</tr>
</tbody>
</table>
</section>
<section id="retrieval-fine-tuning-head">
<h2>Retrieval Fine-tuning (Head)<a class="headerlink" href="#retrieval-fine-tuning-head" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>Note: This section demonstrates how to perform a cost-effective fine-tuning of our model. The hyperparameters shown here are not optimized for any specific task. For practical applications, further adjustments to the hyperparameters may be necessary to achieve optimal performance.</p>
</div></blockquote>
<p>To fine-tune the head of InternVL on Flickr30K with 4 GPUs, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">GPUS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="nv">BATCH_SIZE</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>sh<span class="w"> </span>shell/head_finetune/internvl_stage2_finetune_flickr_224_bs1024_ep10_head_4gpu.sh
</pre></div>
</div>
<p>To fine-tune the head of InternVL on Flickr30K-CN with 4 GPUs, run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">GPUS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="nv">BATCH_SIZE</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>sh<span class="w"> </span>shell/head_finetune/internvl_stage2_finetune_flickrcn_224_bs1024_ep10_head_4gpu.sh
</pre></div>
</div>
<p>To fine-tune the head of InternVL on COCO with 4 GPUs, run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">GPUS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="nv">BATCH_SIZE</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>shell/head_finetune/internvl_stage2_finetune_coco_224_bs1024_ep5_head_4gpu.sh
</pre></div>
</div>
<p>The hyperparameters used here are:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>config</p></th>
<th class="head"><p>Flickr30K</p></th>
<th class="head"><p>Flickr30K-CN</p></th>
<th class="head"><p>COCO</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>learning rate</p></td>
<td><p>1e-6</p></td>
<td><p>1e-6</p></td>
<td><p>1e-6</p></td>
</tr>
<tr class="row-odd"><td><p>optimizer</p></td>
<td><p>AdamW</p></td>
<td><p>AdamW</p></td>
<td><p>AdamW</p></td>
</tr>
<tr class="row-even"><td><p>weight decay</p></td>
<td><p>0.05</p></td>
<td><p>0.05</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-odd"><td><p>input resolution</p></td>
<td><p>224x224</p></td>
<td><p>224x224</p></td>
<td><p>224x224</p></td>
</tr>
<tr class="row-even"><td><p>total batch size</p></td>
<td><p>4x32</p></td>
<td><p>4x32</p></td>
<td><p>4x32</p></td>
</tr>
<tr class="row-odd"><td><p>warm-up iterations</p></td>
<td><p>100</p></td>
<td><p>100</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-even"><td><p>training epochs</p></td>
<td><p>10</p></td>
<td><p>10</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-odd"><td><p>drop path rate</p></td>
<td><p>0.0</p></td>
<td><p>0.0</p></td>
<td><p>0.3</p></td>
</tr>
<tr class="row-even"><td><p>numerical precision</p></td>
<td><p>zero3 + bf16</p></td>
<td><p>zero3 + bf16</p></td>
<td><p>zero1 + bf16</p></td>
</tr>
<tr class="row-odd"><td><p>trainable / total params</p></td>
<td><p>0.2B / 14B</p></td>
<td><p>0.2B / 14B</p></td>
<td><p>0.2B / 14B</p></td>
</tr>
<tr class="row-even"><td><p>GPUs for training</p></td>
<td><p>4×GPU (&gt;=32G)</p></td>
<td><p>4×GPU (&gt;=32G)</p></td>
<td><p>4×GPU (&gt;=32G)</p></td>
</tr>
<tr class="row-odd"><td><p>Required GPU memory</p></td>
<td><p>24G</p></td>
<td><p>24G</p></td>
<td><p>24G</p></td>
</tr>
</tbody>
</table>
</section>
<section id="retrieval-fine-tuning-lora">
<h2>Retrieval Fine-tuning (LoRA)<a class="headerlink" href="#retrieval-fine-tuning-lora" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>Note: This section demonstrates how to perform a cost-effective fine-tuning of our model. The hyperparameters shown here are not optimized for any specific task. For practical applications, further adjustments to the hyperparameters may be necessary to achieve optimal performance.</p>
</div></blockquote>
<p>To fine-tune InternVL using LoRA on Flickr30K with 4 GPUs, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">GPUS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="nv">BATCH_SIZE</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>sh<span class="w"> </span>shell/lora_finetune/internvl_stage2_finetune_flickr_224_bs1024_ep10_lora16_4gpu.sh
</pre></div>
</div>
<p>To fine-tune InternVL using LoRA on Flickr30K-CN with 4 GPUs, run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">GPUS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="nv">BATCH_SIZE</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>sh<span class="w"> </span>shell/lora_finetune/internvl_stage2_finetune_flickrcn_224_bs1024_ep10_lora16_4gpu.sh
</pre></div>
</div>
<p>To fine-tune InternVL using LoRA on COCO with 4 GPUs, run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">GPUS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="nv">BATCH_SIZE</span><span class="o">=</span><span class="m">32</span><span class="w"> </span>shell/lora_finetune/internvl_stage2_finetune_coco_224_bs1024_ep5_lora16_4gpu.sh
</pre></div>
</div>
<p>The hyperparameters used here are:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>config</p></th>
<th class="head"><p>Flickr30K</p></th>
<th class="head"><p>Flickr30K-CN</p></th>
<th class="head"><p>COCO</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>learning rate</p></td>
<td><p>1e-6</p></td>
<td><p>1e-6</p></td>
<td><p>1e-6</p></td>
</tr>
<tr class="row-odd"><td><p>optimizer</p></td>
<td><p>AdamW</p></td>
<td><p>AdamW</p></td>
<td><p>AdamW</p></td>
</tr>
<tr class="row-even"><td><p>lora rank</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
</tr>
<tr class="row-odd"><td><p>weight decay</p></td>
<td><p>0.05</p></td>
<td><p>0.05</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-even"><td><p>input resolution</p></td>
<td><p>224x224</p></td>
<td><p>224x224</p></td>
<td><p>224x224</p></td>
</tr>
<tr class="row-odd"><td><p>total batch size</p></td>
<td><p>4x32</p></td>
<td><p>4x32</p></td>
<td><p>4x32</p></td>
</tr>
<tr class="row-even"><td><p>warm-up iterations</p></td>
<td><p>100</p></td>
<td><p>100</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-odd"><td><p>training epochs</p></td>
<td><p>10</p></td>
<td><p>10</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-even"><td><p>drop path rate</p></td>
<td><p>0.0</p></td>
<td><p>0.0</p></td>
<td><p>0.3</p></td>
</tr>
<tr class="row-odd"><td><p>numerical precision</p></td>
<td><p>zero3 + bf16</p></td>
<td><p>zero3 + bf16</p></td>
<td><p>zero1 + bf16</p></td>
</tr>
<tr class="row-even"><td><p>trainable / total params</p></td>
<td><p>0.3B / 14B</p></td>
<td><p>0.3B / 14B</p></td>
<td><p>0.3B / 14B</p></td>
</tr>
<tr class="row-odd"><td><p>GPUs for training</p></td>
<td><p>4×GPU (&gt;=40G)</p></td>
<td><p>4×GPU (&gt;=40G)</p></td>
<td><p>4×GPU (&gt;=40G)</p></td>
</tr>
<tr class="row-even"><td><p>Required GPU memory</p></td>
<td><p>37G</p></td>
<td><p>37G</p></td>
<td><p>37G</p></td>
</tr>
</tbody>
</table>
</section>
<section id="fine-tuning-a-custom-dataset">
<h2>Fine-Tuning a Custom Dataset<a class="headerlink" href="#fine-tuning-a-custom-dataset" title="Permalink to this heading">#</a></h2>
<ol class="arabic">
<li><p><strong>Organize Your Data</strong>: Format your dataset similar to COCO or Flickr30K.</p></li>
<li><p><strong>Update Meta Information</strong>: Add your dataset’s meta information to the <code class="docutils literal notranslate"><span class="pre">ds_collections</span></code> dictionary in <code class="docutils literal notranslate"><span class="pre">internvl_g/internvl/train/internvl_stage2_finetune.py</span></code>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ds_collections</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;my_dataset_flickr_format&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;root&#39;</span><span class="p">:</span> <span class="s1">&#39;./data/my_dataset/images/&#39;</span><span class="p">,</span>
        <span class="s1">&#39;annotation&#39;</span><span class="p">:</span> <span class="s1">&#39;./data/my_dataset/annotations.txt&#39;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s1">&#39;my_dataset_coco_format&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;root&#39;</span><span class="p">:</span> <span class="s1">&#39;./data/my_dataset/&#39;</span><span class="p">,</span>
        <span class="s1">&#39;annotation&#39;</span><span class="p">:</span> <span class="s1">&#39;./data/my_dataset/annotations.json&#39;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p><strong>Name Your Dataset</strong>:</p>
<ul class="simple">
<li><p>Include <code class="docutils literal notranslate"><span class="pre">flickr_format</span></code> or <code class="docutils literal notranslate"><span class="pre">coco_format</span></code> in your dataset’s <code class="docutils literal notranslate"><span class="pre">dataset_name</span></code>. This will allow the script to reuse the Flickr30K or COCO dataloader accordingly.</p></li>
</ul>
</li>
</ol>
<p>By following these steps, you can easily fine-tune the InternVL model on your custom dataset using the existing COCO or Flickr30K data loading mechanisms.</p>
</section>
<section id="citation">
<h2>Citation<a class="headerlink" href="#citation" title="Permalink to this heading">#</a></h2>
<p>If you find this project useful in your research, please consider citing:</p>
<div class="highlight-BibTeX notranslate"><div class="highlight"><pre><span></span><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2023internvl</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2312.14238}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
<span class="nc">@article</span><span class="p">{</span><span class="nl">chen2024far</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2404.16821}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</pre></div>
</div>
<br>
<br>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="internvl_chat_llava.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">InternVL for Multimodal Dialogue using LLaVA Codebase</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">Data Preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-preparation">Model Preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-pre-training">Generative Pre-training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-image-captioning">Zero-Shot Image Captioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuned-image-text-retrieval">Fine-tuned Image-Text Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flickr30k-fine-tuned-model-internvl-14b-flickr30k-ft-364px">Flickr30K fine-tuned model: InternVL-14B-Flickr30K-FT-364px</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flickr30k-cn-fine-tuned-model-internvl-14b-flickrcn-ft-364px">Flickr30K-CN fine-tuned model: InternVL-14B-FlickrCN-FT-364px</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-fine-tuning-fully">Retrieval Fine-tuning (Fully)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-fine-tuning-head">Retrieval Fine-tuning (Head)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-fine-tuning-lora">Retrieval Fine-tuning (LoRA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-a-custom-dataset">Fine-Tuning a Custom Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citation">Citation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By InternVL Authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, OpenGVLab.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Aug 06, 2024.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>