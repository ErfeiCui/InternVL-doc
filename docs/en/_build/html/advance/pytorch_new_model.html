

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>How to support new model in lmdeploy.pytorch &#8212; internvl</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/readthedocs.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'advance/pytorch_new_model';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Jul 24, 2024"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/internvl-logo.svg" class="logo__image only-light" alt="internvl - Home"/>
    <script>document.write(`<img src="../_static/internvl-logo.svg" class="logo__image only-dark" alt="internvl - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Get Started</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternVL-Chat</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../build.html">Build from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternVL-Chat-LLaVA</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../build.html">Build from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classification</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../build.html">Build from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CLIP Benchmark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../build.html">Build from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternVL-G</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../build.html">Build from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Segmentation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../internvl1.0/segmentation.html">InternViT-6B for Segmentation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/InternLM/internvl" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/InternLM/internvl/blob/main/docs/en/advance/pytorch_new_model.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/InternLM/internvl/edit/main/docs/en/advance/pytorch_new_model.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/InternLM/internvl/issues/new?title=Issue%20on%20page%20%2Fadvance/pytorch_new_model.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/advance/pytorch_new_model.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>How to support new model in lmdeploy.pytorch</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-new-model">Support New Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-tensor-parallelism">Support Tensor Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-module">Debug Module</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-info">context info</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#faq">FAQ</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="how-to-support-new-model-in-lmdeploy-pytorch">
<h1>How to support new model in lmdeploy.pytorch<a class="headerlink" href="#how-to-support-new-model-in-lmdeploy-pytorch" title="Permalink to this heading">#</a></h1>
<p>lmdeploy.pytorch is designed to ease new model deployment and prototype verification. If you are willing to use our engine, here is the tutorial.</p>
<section id="support-new-model">
<h2>Support New Model<a class="headerlink" href="#support-new-model" title="Permalink to this heading">#</a></h2>
<p>Let’s begin with Llama.</p>
<p>Before delving into the details, it’s essential to acquaint ourselves with the input specifications of the model. In order to accommodate new features within our engine, there are some deviations from the typical transformer inputs.</p>
<ol class="arabic simple">
<li><p>To circumvent the need for batch padding, continuous batching is employed. Consequently, the <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> now represents the concatenation of all input sequences in the batch, followed by a <code class="docutils literal notranslate"><span class="pre">unsqueeze(0)</span></code> operation to align with the original <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> dimension.</p></li>
<li><p>In an effort to optimize memory usage for the key/value cache, we implement paged attention. This transforms the <code class="docutils literal notranslate"><span class="pre">past_key_value</span></code> into a substantial tensor with dimensions <code class="docutils literal notranslate"><span class="pre">[num_blocks,</span> <span class="pre">block_size,</span> <span class="pre">num_heads,</span> <span class="pre">head_dim]</span></code>. Here, <code class="docutils literal notranslate"><span class="pre">num_blocks</span></code> denotes the number of page blocks, and <code class="docutils literal notranslate"><span class="pre">block_size</span></code> indicates the size of each block.</p></li>
<li><p>Accompanying these changes, additional inputs are imperative to support the modified inputs described above. These include the block table and history length. It’s important to note that these supplementary inputs are not explicitly listed as arguments in the original forward method. Instead, a context object is utilized to furnish this essential information.</p></li>
</ol>
<p>Due to the alterations in the input structure mentioned earlier, the forward methods for both <code class="docutils literal notranslate"><span class="pre">LlamaModel</span></code> and <code class="docutils literal notranslate"><span class="pre">LlamaAttention</span></code> modules need to be adjusted. Below are the modified implementations:</p>
<p>For <code class="docutils literal notranslate"><span class="pre">LlamaModel</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># lmdeploy/pytorch/models/llama.py</span>

<span class="k">class</span> <span class="nc">LlamaModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutputWithPast</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Rewrite implementation of LlamaModel.forward.&quot;&quot;&quot;</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span>

        <span class="c1"># decoder layers</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">past_key_value</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
                <span class="n">past_key_value</span><span class="o">=</span><span class="n">past_key_value</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithPast</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>For LlamaAttention:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># lmdeploy/pytorch/models/llama.py</span>
<span class="kn">from</span> <span class="nn">lmdeploy.pytorch.kernels</span> <span class="kn">import</span> <span class="n">apply_rotary_pos_emb</span><span class="p">,</span> <span class="n">fill_kv_cache</span><span class="p">,</span> <span class="n">paged_attention_fwd</span>

<span class="k">class</span> <span class="nc">LlamaAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
               <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Rewrite of LlamaAttention.forward.&quot;&quot;&quot;</span>
        <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">context</span>
        <span class="n">history_lengths</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">history_lengths</span>
        <span class="n">position_ids_1d</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">position_ids_1d</span>
        <span class="n">block_offsets</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">block_offsets</span>

        <span class="c1"># qkv proj</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># rotary embedding</span>
        <span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">kv_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span> <span class="o">+</span> <span class="nb">max</span><span class="p">(</span><span class="n">history_lengths</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">kv_seq_len</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="o">.</span><span class="n">max_seq_len_cached</span><span class="p">:</span>
            <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span>
                                        <span class="n">seq_len</span><span class="o">=</span><span class="n">kv_seq_len</span> <span class="o">+</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span>
            <span class="n">query_states</span><span class="p">,</span>
            <span class="n">key_states</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="o">.</span><span class="n">cos_cached</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="o">.</span><span class="n">sin_cached</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">,</span>
            <span class="n">position_ids_1d</span><span class="p">,</span>
            <span class="n">q_embed</span><span class="o">=</span><span class="n">query_states</span><span class="p">,</span>
            <span class="n">k_embed</span><span class="o">=</span><span class="n">key_states</span><span class="p">)</span>

        <span class="c1"># fill kv cache</span>
        <span class="n">kv_seq_length</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">kv_seq_length</span>
        <span class="n">q_seq_length</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">q_seq_length</span>
        <span class="n">q_start_loc</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">q_start_loc</span>
        <span class="n">fill_kv_cache</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span>
                      <span class="n">value_states</span><span class="p">,</span>
                      <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                      <span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                      <span class="n">q_start_loc</span><span class="p">,</span>
                      <span class="n">q_seq_length</span><span class="p">,</span>
                      <span class="n">block_offsets</span><span class="o">=</span><span class="n">block_offsets</span><span class="p">,</span>
                      <span class="n">history_lengths</span><span class="o">=</span><span class="n">history_lengths</span><span class="p">,</span>
                      <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>

        <span class="c1"># attention</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">query_states</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">paged_attention_fwd</span><span class="p">(</span>
            <span class="n">query_states</span><span class="p">,</span>
            <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">attn_output</span><span class="p">,</span>
            <span class="n">block_offsets</span><span class="p">,</span>
            <span class="n">q_start_loc</span><span class="o">=</span><span class="n">q_start_loc</span><span class="p">,</span>
            <span class="n">q_seqlens</span><span class="o">=</span><span class="n">q_seq_length</span><span class="p">,</span>
            <span class="n">kv_seqlens</span><span class="o">=</span><span class="n">kv_seq_length</span><span class="p">,</span>
            <span class="n">max_seqlen</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_size</span><span class="p">)</span>

        <span class="c1"># o proj</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span>
</pre></div>
</div>
<p>Note: The additional arguments like <code class="docutils literal notranslate"><span class="pre">history_lengths</span></code> and <code class="docutils literal notranslate"><span class="pre">block_offsets</span></code> are accessed from the <code class="docutils literal notranslate"><span class="pre">context</span></code> object, which acts as a container for the necessary inputs required by continuous batching and paged attention. Refer to the <a class="reference internal" href="#context-info">context info</a> for more detail about <code class="docutils literal notranslate"><span class="pre">context</span></code> object.</p>
<p>We have replaced certain operations with our custom Triton kernel for two reasons:</p>
<ol class="arabic simple">
<li><p>The custom Triton kernel allows us to incorporate new features, such as <code class="docutils literal notranslate"><span class="pre">paged_attention_fwd</span></code>.</p></li>
<li><p>Fused kernels offer superior performance compared to the pure PyTorch implementation.</p></li>
</ol>
<p>Now that we have the updated implementations for the two modules, let’s register them in <code class="docutils literal notranslate"><span class="pre">lmdeploy/pytorch/models/module_map.py</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># lmdeploy/pytorch/models/module_map.py</span>
<span class="n">MODEL_MAP</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
    <span class="s1">&#39;transformers.models.llama.modeling_llama.LlamaAttention&#39;</span><span class="p">:</span>
    <span class="s1">&#39;lmdeploy.pytorch.models.llama.LlamaAttention&#39;</span><span class="p">,</span>
    <span class="s1">&#39;transformers.models.llama.modeling_llama.LlamaModel&#39;</span><span class="p">:</span>
    <span class="s1">&#39;lmdeploy.pytorch.models.llama.LlamaModel&#39;</span>
<span class="p">})</span>
</pre></div>
</div>
<p>In this mapping, the revised modules are associated with their original counterparts. When creating an <code class="docutils literal notranslate"><span class="pre">Engine</span></code>, the <code class="docutils literal notranslate"><span class="pre">ModelAgent</span></code> will automatically patch the model. Subsequently, we can conduct inference using these updated implementations.</p>
</section>
<section id="support-tensor-parallelism">
<h2>Support Tensor Parallelism<a class="headerlink" href="#support-tensor-parallelism" title="Permalink to this heading">#</a></h2>
<p>If we aim to enable tensor parallelism (TP), it is necessary to partition the weights in the model. Let’s build upon the previously mentioned modifications to accommodate TP in the Llama model:</p>
<p>In Llama (as well as in most Language Model models), the weight partition primarily affects the Linear layers. Specifically, for the following components:</p>
<ul class="simple">
<li><p>In <code class="docutils literal notranslate"><span class="pre">LlamaAttention</span></code>: <code class="docutils literal notranslate"><span class="pre">q_proj</span></code>, <code class="docutils literal notranslate"><span class="pre">k_proj</span></code>, <code class="docutils literal notranslate"><span class="pre">v_proj</span></code> require column-wise partitioning, while <code class="docutils literal notranslate"><span class="pre">o_proj</span></code> necessitates row-wise partitioning.</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">LlamaMLP</span></code>: <code class="docutils literal notranslate"><span class="pre">gate_proj</span></code> and <code class="docutils literal notranslate"><span class="pre">up_proj</span></code> require column-wise partitioning, while <code class="docutils literal notranslate"><span class="pre">down_proj</span></code> requires row-wise partitioning.</p></li>
</ul>
<p>We can implement the _distribution_partition_fn in each of the rewritten modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># lmdeploy/pytorch/models/llama.py</span>
<span class="kn">from</span> <span class="nn">..dist_utils</span> <span class="kn">import</span> <span class="p">(</span><span class="n">colwise_parallelize_linear_fn</span><span class="p">,</span>
                          <span class="n">rowwise_parallelize_linear_fn</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LlamaAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_distribute_partition_fn</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">mod_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                                 <span class="n">device_mesh</span><span class="p">:</span> <span class="n">DeviceMesh</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Distribution partition callback.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mod_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;q_proj&#39;</span><span class="p">,</span> <span class="s1">&#39;k_proj&#39;</span><span class="p">,</span> <span class="s1">&#39;v_proj&#39;</span><span class="p">]:</span>
            <span class="n">colwise_parallelize_linear_fn</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span>
                                          <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">,</span>
                                          <span class="n">to_local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mod_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;o_proj&#39;</span><span class="p">]:</span>
            <span class="n">rowwise_parallelize_linear_fn</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span>
                                          <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">,</span>
                                          <span class="n">to_local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LlamaMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_distribute_partition_fn</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">mod_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                                 <span class="n">device_mesh</span><span class="p">:</span> <span class="n">DeviceMesh</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Distribution partition callback.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mod_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gate_proj&#39;</span><span class="p">,</span> <span class="s1">&#39;up_proj&#39;</span><span class="p">]:</span>
            <span class="n">colwise_parallelize_linear_fn</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span>
                                          <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">,</span>
                                          <span class="n">to_local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mod_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;down_proj&#39;</span><span class="p">]:</span>
            <span class="n">rowwise_parallelize_linear_fn</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span>
                                          <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">,</span>
                                          <span class="n">to_local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

</pre></div>
</div>
<p>In the process of loading model weights, the <code class="docutils literal notranslate"><span class="pre">_distribute_partition_fn</span></code> is called to distribute the weights of specific modules across different devices. Following the weight partitioning, it becomes necessary to perform <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code> on the output tensors of <code class="docutils literal notranslate"><span class="pre">o_proj</span></code> and <code class="docutils literal notranslate"><span class="pre">down_proj</span></code>. While one option is to include <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code> directly in the forward method, an alternative approach is to introduce the <code class="docutils literal notranslate"><span class="pre">_distribute_output_fn</span></code> call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># lmdeploy/pytorch/models/llama.py</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">class</span> <span class="nc">LlamaAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_distribute_output_fn</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">:</span> <span class="n">DeviceMesh</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Distribution output hook.&quot;&quot;&quot;</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">outputs</span>

<span class="k">class</span> <span class="nc">LlamaMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_distribute_output_fn</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">:</span> <span class="n">DeviceMesh</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Distribution output hook.&quot;&quot;&quot;</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>
</div>
<p>It is essential to remember to add <code class="docutils literal notranslate"><span class="pre">LlamaMLP</span></code> to the <code class="docutils literal notranslate"><span class="pre">module_map</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># lmdeploy/pytorch/models/module_map.py</span>
<span class="n">MODEL_MAP</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
    <span class="s1">&#39;transformers.models.llama.modeling_llama.LlamaMLP&#39;</span><span class="p">:</span>
    <span class="s1">&#39;lmdeploy.pytorch.models.llama.LlamaMLP&#39;</span>
<span class="p">})</span>
</pre></div>
</div>
<p>With these adjustments, the model is now capable of utilizing multiple GPUs for deploying Large Language Models (LLM). This enables efficient distribution of computations across different devices in a parallelized manner.</p>
</section>
<section id="debug-module">
<h2>Debug Module<a class="headerlink" href="#debug-module" title="Permalink to this heading">#</a></h2>
<p>When the output of the model does not meet expectations, we would like to debug a specific module to determine if the added rewrite is correct. <code class="docutils literal notranslate"><span class="pre">lmdeploy.pytorch</span></code> provides some tools to assist with accuracy alignment. Let’s take <code class="docutils literal notranslate"><span class="pre">LlamaAttention</span></code> module as an example.</p>
<p>First, create an instance of the module that we want to debug:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># get module</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;meta-llama/Llama-2-7b-chat-hf&#39;</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">self_attn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">self_attn</span>
</pre></div>
</div>
<p>Extract the inputs/outputs with <code class="docutils literal notranslate"><span class="pre">ModuleIOExtractor</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lmdeploy.pytorch.tools.make_inputs</span> <span class="kn">import</span> <span class="n">ModuleIOExtractor</span>

<span class="c1"># extract module input/output</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">extractor</span> <span class="o">=</span> <span class="n">ModuleIOExtractor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">)</span>
<span class="n">attn_args</span><span class="p">,</span> <span class="n">attn_kwargs</span><span class="p">,</span> <span class="n">attn_output</span> <span class="o">=</span> <span class="n">extractor</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
</pre></div>
</div>
<p>The inputs of rewrite module are different from the inputs of origin module:</p>
<ol class="arabic simple">
<li><p>Module requires some special inputs, which are passed through <code class="docutils literal notranslate"><span class="pre">StepContext</span></code>. We can create one with <code class="docutils literal notranslate"><span class="pre">make_step_context</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> should be continuous. We can use <code class="docutils literal notranslate"><span class="pre">continuous_tensor</span></code> to do the process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">past_key_value</span></code> should be paged to meet the demand of paged attention.</p></li>
</ol>
<p>Based on the reason above, the input should be updated:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lmdeploy.pytorch.tools.make_inputs</span> <span class="kn">import</span> <span class="n">make_step_context</span>
<span class="kn">from</span> <span class="nn">lmdeploy.pytorch.tools.layout_convert</span> <span class="kn">import</span> <span class="n">continuous_tensor</span>

<span class="c1"># create patched input/output</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">make_step_context</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span>
                            <span class="n">kv_cache_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                            <span class="n">num_key_value_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">q_seq_length</span>
<span class="n">attn_kwargs</span><span class="p">[</span><span class="s1">&#39;hidden_states&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">continuous_tensor</span><span class="p">(</span>
    <span class="n">attn_kwargs</span><span class="p">[</span><span class="s1">&#39;hidden_states&#39;</span><span class="p">],</span>
    <span class="n">seq_length</span><span class="p">)</span>
<span class="n">attn_kwargs</span><span class="p">[</span><span class="s1">&#39;past_key_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">kv_caches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Then you can start the rewrite and compare the correctness of the results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lmdeploy.pytorch.models</span> <span class="kn">import</span> <span class="n">patch</span>

<span class="c1"># patch and test</span>
<span class="n">patched_self_attn</span> <span class="o">=</span> <span class="n">patch</span><span class="p">(</span><span class="n">self_attn</span><span class="p">,</span> <span class="n">extra_args</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;context&#39;</span><span class="p">])</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">patched_output</span> <span class="o">=</span> <span class="n">patched_self_attn</span><span class="o">.</span><span class="n">patched_forward</span><span class="p">(</span><span class="o">*</span><span class="n">attn_args</span><span class="p">,</span>
                                                       <span class="o">**</span><span class="n">attn_kwargs</span><span class="p">,</span>
                                                       <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">patched_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">continuous_tensor</span><span class="p">(</span><span class="n">attn_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">seq_length</span><span class="p">))</span>
</pre></div>
</div>
<p>Adjust the rewrite module until the output can be aligned.</p>
</section>
<section id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this heading">#</a></h2>
<section id="context-info">
<h3>context info<a class="headerlink" href="#context-info" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">StepContext</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;context of Model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">ModelInputs</span>
    <span class="n">block_offsets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
    <span class="n">position_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
    <span class="n">position_ids_1d</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
    <span class="n">q_start_loc</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
    <span class="n">history_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
    <span class="n">seq_length</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
    <span class="n">max_seq_length</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">kv_seq_length</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
    <span class="n">kv_caches</span><span class="p">:</span> <span class="n">List</span>
    <span class="n">is_decoding</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">json_config</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">local_adapter_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">global_adapter_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">adapter_offsets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">max_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</section>
<section id="faq">
<h3>FAQ<a class="headerlink" href="#faq" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>How to invoke the original forward method?</strong></p></li>
</ul>
<p>A common approach is to add hooks to a method rather than performing a complete rewrite. To access the unpatched module, you can utilize self.origin_mod within the rewritten method.</p>
<ul class="simple">
<li><p><strong>How to register modules in remote code?</strong></p></li>
</ul>
<p>For modules located in remote code, pinpointing them via <code class="docutils literal notranslate"><span class="pre">qualname</span></code> might be challenging. <code class="docutils literal notranslate"><span class="pre">lmdeploy.pytorch</span></code> facilitates registration using abbreviations for such modules:n:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MODULE_MAP</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
    <span class="s1">&#39;modeling_internlm.InternLMAttention&#39;</span><span class="p">:</span>
    <span class="s1">&#39;lmdeploy.pytorch.models.internlm.PatchedInternLMAttention&#39;</span><span class="p">,</span>
<span class="p">})</span>
</pre></div>
</div>
<blockquote>
<div><p>[!NOTE]</p>
<p>Although abbreviations are supported, they tend to have lower priority. It is advisable to register modules using their complete <code class="docutils literal notranslate"><span class="pre">qualname</span></code> for more robust and accurate mapping.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>How to support different modules with the same name?</strong></p></li>
</ul>
<p>You can accommodate multiple modules with the same name within a single rewrite module by providing distinct implementations based on their attributes. For instance, consider <code class="docutils literal notranslate"><span class="pre">baichuan2</span></code> 7b/13b:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaichuanModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">==</span> <span class="mi">32</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">forward_7b</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">forward_default</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>How to perform post-initialization for a rewrite module?</strong></p></li>
</ul>
<p>To execute tasks after model weight loading, introduce a <code class="docutils literal notranslate"><span class="pre">_update_model_fn</span></code> method in your rewrite module. This method will be automatically called post-initialization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LlamaAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_update_model_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># ADD YOUR CODE HERE</span>
</pre></div>
</div>
<p>Here, you can include any additional post-initialization steps or configurations needed for your specific use case.</p>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-new-model">Support New Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-tensor-parallelism">Support Tensor Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-module">Debug Module</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-info">context info</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#faq">FAQ</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By InternVL Authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, OpenGVLab.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Jul 24, 2024.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>